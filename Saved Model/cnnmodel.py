# -*- coding: utf-8 -*-
"""CNNModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xL6Pnq9oWXnYDdHtKp6mpl_2dNrvR7Br
"""

from google.colab import drive
drive.mount('drive')

import IPython.display as ipd
# % pylab inline
import os
import pandas as pd
import librosa
import glob 
import librosa.display
import random

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from tensorflow.keras.utils import to_categorical

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Convolution2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.utils import np_utils
from sklearn import metrics 

from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.model_selection import train_test_split, GridSearchCV

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout 
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

# from keras.callbacks import EarlyStopping

from tensorflow.keras import regularizers

from sklearn.preprocessing import LabelEncoder

from datetime import datetime

import os


import tensorflow

from sklearn import preprocessing
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import Conv1D, MaxPooling1D
from tensorflow.keras.layers import Flatten, Dropout, Activation
import matplotlib.pyplot as plt
from tensorflow.keras.models import model_from_json

dfData = pd.read_csv('drive/My Drive/VoiceData/all_Data.csv')

dfData.head()

X = dfData.loc[:, dfData.columns != 'label']
y = dfData['label']

lb = preprocessing.LabelEncoder()
y = lb.fit_transform(y)

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()

scaler.fit(X)

x = scaler.transform(X)

dfData['encode'] = y

dfData.to_csv('drive/My Drive/VoiceData/DataSet.csv',index=None)

X = np.array(X)

X_trainn, X_test, y_trainn, y_test = train_test_split(x, y, test_size=0.30, random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_trainn, y_trainn, test_size=0.30, random_state=5)

len(pd.unique(y))

X_trainn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_testt = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))
X_vall = np.reshape(X_val, (X_val.shape[0], X_val.shape[1],1))

modelcnn = Sequential()

modelcnn.add(Conv1D(512, 3,padding='same',
                 input_shape=(X_train.shape[1],1)))
modelcnn.add(Activation('tanh'))
modelcnn.add(Conv1D(256, 3,padding='same'))
modelcnn.add(Activation('tanh'))
modelcnn.add(Dropout(0.5))

modelcnn.add(Conv1D(128, 3,padding='same',))
modelcnn.add(Activation('tanh'))
modelcnn.add(Conv1D(512, 3,padding='same',))
modelcnn.add(Dropout(0.5))
modelcnn.add(Activation('tanh'))
modelcnn.add(Flatten())
modelcnn.add(Dense(len(pd.unique(y))))
modelcnn.add(Activation('softmax'))
opt = tensorflow.keras.optimizers.Adam(lr=0.00001, decay=1e-6)

modelcnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt,metrics=['accuracy'])

cnnhistory=modelcnn.fit(X_trainn, y_train, batch_size=16, epochs=50, validation_data=(X_vall, y_val))

# Check out our train accuracy and validation accuracy over epochs.
train_accuracy = cnnhistory.history['accuracy']
val_accuracy = cnnhistory.history['val_accuracy']

# Set figure size.
plt.figure(figsize=(20, 8))

# Generate line plot of training, testing loss over epochs.
plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')
plt.plot(val_accuracy, label='Validation Accuracy', color='red')
# plt.plot(train_loss, label='Training Loss', color='#185fad')
# plt.plot(val_loss, label='Validation Loss', color='red')
# Set title
plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)
plt.xlabel('Epoch', fontsize = 18)
plt.ylabel('Accuracy', fontsize = 18)
plt.yticks(fontsize = 15)

plt.xticks(range(0,50,1), range(0,50,1),fontsize = 12)

plt.legend(fontsize = 18);

train_loss = cnnhistory.history['loss']
val_loss = cnnhistory.history['val_loss']

# Set figure size.
plt.figure(figsize=(20, 8))

# Generate line plot of training, testing loss over epochs.
plt.plot(train_loss, label='Training Loss', color='#185fad')
plt.plot(val_loss, label='Validation Loss', color='red')

# Set title
plt.title('Training and Validation Loss by Epoch', )
plt.xlabel('Epoch', fontsize = 18)
plt.ylabel('Loss', fontsize = 18)
plt.yticks(fontsize = 15)
plt.xticks(range(0,50,1), range(0,50,1),fontsize = 12)

plt.legend(fontsize = 18);

modelcnn.evaluate(X_testt,y_test, batch_size=16)

model_json = modelcnn.to_json()

with open("drive/My Drive/VoiceData/modelcnn.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
modelcnn.save_weights("drive/My Drive/VoiceData/modelcnn.h5")
print("Saved model to disk")

json_file = open('drive/My Drive/VoiceData/modelcnn.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("drive/My Drive/VoiceData/modelcnn.h5")
print("Loaded model from disk")
loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer=opt,metrics=['accuracy'])

import tensorflow as tf; 
print(tf.__version__) # for Python 3

